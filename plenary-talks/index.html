<!DOCTYPE html>
<html>
<head>
    
        
<meta charset="utf-8">
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer-when-downgrade">

<title>Plenary Talks  - Mathematics of Data Science</title>
<meta name="description" content="Mathematics of Data Science.">

<link rel="alternate" type="application/rss+xml" title="RSS" href="https://maths-of-data.github.io/rss/">

<link rel="icon" type="image/x-icon" href="https://maths-of-data.github.io/favicon.ico">
<link rel="apple-touch-icon-precomposed" href="https://maths-of-data.github.io/favicon.png">

<link rel="stylesheet" href="https://maths-of-data.github.io/css/style.css?rnd=1591781368" />

<meta property="og:title" content="Plenary Talks" />
<meta property="og:description" content="The Mathematics of Deep Learning: Can we Open the Black Box of Deep Neural Networks? Gitta Kutyniok, Technische Universität Berlin
Despite the outstanding success of deep neural networks in real-world applications, most of the related research is empirically driven and a comprehensive mathematical foundation is still missing. Regarding deep learning as a statistical learning problem, the necessary theory can be divided into the research directions of expressivity, learning, and generalization." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://maths-of-data.github.io/plenary-talks/" />
<meta property="article:published_time" content="2020-06-09T17:07:39+02:00" />
<meta property="article:modified_time" content="2020-06-09T17:07:39+02:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Plenary Talks"/>
<meta name="twitter:description" content="The Mathematics of Deep Learning: Can we Open the Black Box of Deep Neural Networks? Gitta Kutyniok, Technische Universität Berlin
Despite the outstanding success of deep neural networks in real-world applications, most of the related research is empirically driven and a comprehensive mathematical foundation is still missing. Regarding deep learning as a statistical learning problem, the necessary theory can be divided into the research directions of expressivity, learning, and generalization."/>






<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-164955639-1', 'auto');
	
	ga('send', 'pageview');
}
</script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-164955639-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>




    
</head>
<body>
    <div class="container">
        <header> 
            
                <h1 class="site-header">
    <a href="https://maths-of-data.github.io/">Mathematics of Data Science<br>
    </a>
    <img src="/logo2.png" alt="Logo" width="60%" height="60%" style="margin-right: 5%;margin-left: 5%;display:inline-block">
    
</h1>
<nav>
    
    
    <a class="" href="/" title="">Home</a>
    
    <a class="" href="/program/" title="">Programme</a>
    
    <a class="" href="/posters-talks/" title="">Presentations</a>
    
    <a class="" href="/plenary-talks/" title="">Plenary talks</a>
    
</nav>

            
        </header>
        <main>
            

    <article class="post">
        <header>
            <h1>Plenary Talks</h1>
        </header>
        <div class="content">
            <p> </p>
<h1 id="gitta-kutyniok"><strong>The Mathematics of Deep Learning: Can we Open the Black Box of Deep Neural Networks?</strong></h1>
<p><strong>Gitta Kutyniok</strong>, Technische Universität Berlin</p>
<p>Despite the outstanding success of deep neural networks in real-world
applications, most of the related research is empirically driven and a
comprehensive mathematical foundation is still missing. Regarding deep
learning as a statistical learning problem, the necessary theory can
be divided into the research directions of expressivity, learning, and
generalization. Recently, the new direction of interpretability became
important as well.
In this talk, we will provide an introduction into those four research
foci. We will then delve a bit deeper into the area of expressivity,
namely the approximation capacity of neural network architectures as
one of the most developed mathematical theories so far, and discuss
some recent work. Finally, we will provide a survey about the novel
and highly relevant area of interpretability, which aims at developing
an understanding how a given network reaches decisions, and discuss
the very first mathematically founded approach to this problem.</p>
<h1 id="peter-richtarik"><strong>On Second Order Methods and Randomness</strong></h1>
<p><strong>Peter Richtarik</strong>, King Abdullah University of Science and Technology</p>
<p>We present two new remarkably simple stochastic second-order methods for minimizing the average of a very large number of sufficiently smooth and strongly convex functions. The first is a stochastic variant of Newton&rsquo;s method (SN), and the second is a stochastic variant of cubically regularized Newton&rsquo;s method (SCN). We establish local linear-quadratic convergence results. Unlike existing stochastic variants of second order methods, which require the evaluation of a large number of gradients and/or Hessians in each iteration to guarantee convergence, our methods do not have this shortcoming. For instance, the simplest variants of our methods in each iteration need to compute the gradient and Hessian of a single randomly selected function only. In contrast to most existing stochastic Newton and quasi-Newton methods, our approach guarantees local convergence faster than with first-order oracle and adapts to the problem&rsquo;s curvature. Interestingly, our method is not unbiased, so our theory provides new intuition for designing new stochastic methods.</p>
<h1 id="carola-schonlieb"><strong>Hybrid mathematical and machine learning methods for solving inverse imaging problems - getting the best from both worlds</strong></h1>
<p><strong>Carola Schönlieb</strong>, University of Cambridge</p>
<p>In this talk I will discuss image analysis methods which have both a mathematical modelling and a machine learning (data-driven) component. Mathematical modelling is useful in the presence of prior information about the imaging data and relevant biomarkers, for narrowing down the search space, for highly generalizable image analysis methods, and for guaranteeing desirable properties of image analysis solutions. Machine learning on the other hand is a powerful tool for customising image analysis methods to individual data sets. Their combination is the topic of this talk, furnished with examples for image classification under minimal supervision with an application to chest x-rays and task adapted tomographic reconstruction.</p>
<h1 id="alexandre-bouchard-cote"><strong>Scalable approximation of integrals using non-reversible methods</strong></h1>
<p><strong>Alexandre Bouchard-Côté</strong>, University of British Columbia</p>
<p>How to approximate intractable integrals? This is an old problem which is still a pain point in many disciplines (including mine, Bayesian inference, but also statistical mechanics, computational chemistry, combinatorics, etc).</p>
<p>The vast majority of current work on this problem (HMC, SGLD, variational) is based on mimicking the field of optimization, in particular gradient based methods, and as a consequence focusses on Riemann integrals. This severely limits the applicability of these methods, making them inadequate to the wide range of problems requiring the full expressivity of Lebesgue integrals, for example integrals over phylogenetic tree spaces or other mixed combinatorial-continuous problems arising in networks models, record linkage and feature allocation.</p>
<p>I will describe novel perspectives on the problem of approximating Lebesgue integrals, coming from the nascent field of non-reversible Monte Carlo methods. In particular, I will present an adaptive, non-reversible Parallel Tempering (PT) allowing MCMC exploration of challenging problems such as single cell phylogenetic trees.</p>
<p>By analyzing the behaviour of PT algorithms using a novel asymptotic regime, a sharp divide emerges in the behaviour and performance of reversible versus non-reversible PT schemes: the performance of the former eventually collapses as the number of parallel cores used increases whereas non-reversible benefits from arbitrarily many available parallel cores. These theoretical results are exploited to develop an adaptive scheme approximating the optimal annealing schedule.</p>
<p>My group is also interested in making these advanced non-reversible Monte Carlo methods easily available to data scientists. To do so, we have designed a Bayesian modelling language to perform inference over arbitrary data types using non-reversible, highly parallel algorithms, <a href="https://www.stat.ubc.ca/~bouchard/blang/">link here</a>.</p>

        </div>
        <div class="article-info">
    
        <div class="article-date">9/6/2020</div>
    
    <div class="article-taxonomies">
        
            
    </div>
</div>

    </article>
    


        </main>
        <footer>
            
                <p>© Simon Vary, 2020<br>
Powered by <a target="_blank" href="https://gohugo.io/">Hugo</a>, theme <a target="_blank" href="https://github.com/mitrichius/hugo-theme-anubis">Anubis</a>.
</p>
            
        </footer>
    </div>
</body>
</html>
