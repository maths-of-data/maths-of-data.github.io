<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mathematics of Data Science</title>
    <link>https://maths-of-data.github.io/</link>
    <description>Mathematics of Data Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Mon, 20 Apr 2020 14:27:41 +0200</lastBuildDate>
    
    <atom:link href="https://maths-of-data.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Plenary Talks</title>
      <link>https://maths-of-data.github.io/plenary-talks/</link>
      <pubDate>Tue, 09 Jun 2020 17:07:39 +0200</pubDate>
      
      <guid>https://maths-of-data.github.io/plenary-talks/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
&lt;h1 id=&#34;gitta-kutyniok&#34;&gt;&lt;strong&gt;The Mathematics of Deep Learning: Can we Open the Black Box of Deep Neural Networks? &lt;a href=&#34;https://www.youtube.com/watch?v=TkRsGgTnTl0&#34;&gt;[video]&lt;/a&gt;&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Gitta Kutyniok&lt;/strong&gt;, Technische Universität Berlin&lt;/p&gt;
&lt;p&gt;Despite the outstanding success of deep neural networks in real-world
applications, most of the related research is empirically driven and a
comprehensive mathematical foundation is still missing. Regarding deep
learning as a statistical learning problem, the necessary theory can
be divided into the research directions of expressivity, learning, and
generalization. Recently, the new direction of interpretability became
important as well.
In this talk, we will provide an introduction into those four research
foci. We will then delve a bit deeper into the area of expressivity,
namely the approximation capacity of neural network architectures as
one of the most developed mathematical theories so far, and discuss
some recent work. Finally, we will provide a survey about the novel
and highly relevant area of interpretability, which aims at developing
an understanding how a given network reaches decisions, and discuss
the very first mathematically founded approach to this problem.&lt;/p&gt;
&lt;h1 id=&#34;peter-richtarik&#34;&gt;&lt;strong&gt;On Second Order Methods and Randomness &lt;a href=&#34;https://www.youtube.com/watch?v=LkkWAXzAaYU&#34;&gt;[video]&lt;/a&gt;&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Peter Richtarik&lt;/strong&gt;, King Abdullah University of Science and Technology&lt;/p&gt;
&lt;p&gt;We present two new remarkably simple stochastic second-order methods for minimizing the average of a very large number of sufficiently smooth and strongly convex functions. The first is a stochastic variant of Newton&amp;rsquo;s method (SN), and the second is a stochastic variant of cubically regularized Newton&amp;rsquo;s method (SCN). We establish local linear-quadratic convergence results. Unlike existing stochastic variants of second order methods, which require the evaluation of a large number of gradients and/or Hessians in each iteration to guarantee convergence, our methods do not have this shortcoming. For instance, the simplest variants of our methods in each iteration need to compute the gradient and Hessian of a single randomly selected function only. In contrast to most existing stochastic Newton and quasi-Newton methods, our approach guarantees local convergence faster than with first-order oracle and adapts to the problem&amp;rsquo;s curvature. Interestingly, our method is not unbiased, so our theory provides new intuition for designing new stochastic methods.&lt;/p&gt;
&lt;h1 id=&#34;carola-schonlieb&#34;&gt;&lt;strong&gt;Hybrid mathematical and machine learning methods for solving inverse imaging problems - getting the best from both worlds &lt;a href=&#34;https://www.youtube.com/watch?v=4HOJyETXj5U&#34;&gt;[video]&lt;/a&gt;&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Carola Schönlieb&lt;/strong&gt;, University of Cambridge&lt;/p&gt;
&lt;p&gt;In this talk I will discuss image analysis methods which have both a mathematical modelling and a machine learning (data-driven) component. Mathematical modelling is useful in the presence of prior information about the imaging data and relevant biomarkers, for narrowing down the search space, for highly generalizable image analysis methods, and for guaranteeing desirable properties of image analysis solutions. Machine learning on the other hand is a powerful tool for customising image analysis methods to individual data sets. Their combination is the topic of this talk, furnished with examples for image classification under minimal supervision with an application to chest x-rays and task adapted tomographic reconstruction.&lt;/p&gt;
&lt;h1 id=&#34;alexandre-bouchard-cote&#34;&gt;&lt;strong&gt;Scalable approximation of integrals using non-reversible methods &lt;a href=&#34;https://www.youtube.com/watch?v=gE0iMXVtftk&#34;&gt;[video]&lt;/a&gt;&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Alexandre Bouchard-Côté&lt;/strong&gt;, University of British Columbia&lt;/p&gt;
&lt;p&gt;How to approximate intractable integrals? This is an old problem which is still a pain point in many disciplines (including mine, Bayesian inference, but also statistical mechanics, computational chemistry, combinatorics, etc).&lt;/p&gt;
&lt;p&gt;The vast majority of current work on this problem (HMC, SGLD, variational) is based on mimicking the field of optimization, in particular gradient based methods, and as a consequence focusses on Riemann integrals. This severely limits the applicability of these methods, making them inadequate to the wide range of problems requiring the full expressivity of Lebesgue integrals, for example integrals over phylogenetic tree spaces or other mixed combinatorial-continuous problems arising in networks models, record linkage and feature allocation.&lt;/p&gt;
&lt;p&gt;I will describe novel perspectives on the problem of approximating Lebesgue integrals, coming from the nascent field of non-reversible Monte Carlo methods. In particular, I will present an adaptive, non-reversible Parallel Tempering (PT) allowing MCMC exploration of challenging problems such as single cell phylogenetic trees.&lt;/p&gt;
&lt;p&gt;By analyzing the behaviour of PT algorithms using a novel asymptotic regime, a sharp divide emerges in the behaviour and performance of reversible versus non-reversible PT schemes: the performance of the former eventually collapses as the number of parallel cores used increases whereas non-reversible benefits from arbitrarily many available parallel cores. These theoretical results are exploited to develop an adaptive scheme approximating the optimal annealing schedule.&lt;/p&gt;
&lt;p&gt;My group is also interested in making these advanced non-reversible Monte Carlo methods easily available to data scientists. To do so, we have designed a Bayesian modelling language to perform inference over arbitrary data types using non-reversible, highly parallel algorithms, &lt;a href=&#34;https://www.stat.ubc.ca/~bouchard/blang/&#34;&gt;link here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Presentations</title>
      <link>https://maths-of-data.github.io/posters-talks/</link>
      <pubDate>Mon, 08 Jun 2020 18:23:11 +0200</pubDate>
      
      <guid>https://maths-of-data.github.io/posters-talks/</guid>
      <description>&lt;h1 id=&#34;posters&#34;&gt;Posters&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Bhavan Chahal, &lt;strong&gt;Using deep learning tools to infer house prices from Google Street View Images&lt;/strong&gt; [&lt;a href=&#34;https://maths-of-data.github.io/BhavanChahal-bhavan_chahal_poster.pdf&#34;&gt;pdf&lt;/a&gt;,&lt;a href=&#34;https://www.dropbox.com/s/y2pom0lzsvw8rtw/Bhavan%20Chahal%20%20-%20bhavan_chahal_3_min_talk.mp4?dl=0&#34;&gt;video&lt;/a&gt;,&lt;a href=&#34;https://talky.io/Using_deep_learning_tools_to_infer_house_prices_from_Google_Street_View_Images&#34;&gt;talky.io&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Adam Gothorp, &lt;strong&gt;Inverse problems for Bayesian errors-in-variables regression models&lt;/strong&gt; [&lt;a href=&#34;https://maths-of-data.github.io/Adam_Gothorp_poster_virtual_conference_mathODS_v6.pdf&#34;&gt;pdf&lt;/a&gt;,&lt;a href=&#34;https://www.dropbox.com/s/rrgsag0ku56lo0b/Adam%20Gothorp%20%20-%20Adam_Gothorp_poster_virtual_conference_mathODS.mp4?dl=0&#34;&gt;video&lt;/a&gt;,&lt;a href=&#34;https://talky.io/Inverse_problems_for_Bayesian_errors-in-variables_regression_models&#34;&gt;talky.io&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Florenting Goyens, &lt;strong&gt;Nonlinear matrix recovery&lt;/strong&gt; [&lt;a href=&#34;https://maths-of-data.github.io/Goyens_poster_Nonlinear_matrix_recovery.pdf&#34;&gt;pdf&lt;/a&gt;,&lt;a href=&#34;https://www.dropbox.com/s/e1vuzroc4cjk3s4/Florentin%20Goyens%20-%20Goyens_Nonlinear_matrix_recovery.mp4?dl=0&#34;&gt;video&lt;/a&gt;,&lt;a href=&#34;https://talky.io/Nonlinear_matrix_recovery&#34;&gt;talky.io&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Eric Baruch Gutierrez, &lt;strong&gt;An Example of Primal-Dual Methods for Nonsmooth Large-Scale Machine Learning&lt;/strong&gt; [&lt;a href=&#34;https://www.dropbox.com/s/9qhtb141ltotoyo/Eric_Gutierrez%203_min_poster.mp4?dl=0&#34;&gt;video&lt;/a&gt;,&lt;a href=&#34;https://talky.io/An_Example_of_Primal-Dual_Methods_for_Nonsmooth_Large-Scale_Machine_Learning&#34;&gt;talky.io&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Haoran Ni, &lt;strong&gt;Numerical Estimation of Information Measures&lt;/strong&gt; [&lt;a href=&#34;https://maths-of-data.github.io/HaoranNi-HaoranNi_Poster.pdf&#34;&gt;pdf&lt;/a&gt;,&lt;a href=&#34;https://www.dropbox.com/s/0r3mlnfj1q3n1gi/Haoran%20Ni%20-%20HaoranNi_Poster_Presenting.mov?dl=0&#34;&gt;video&lt;/a&gt;,&lt;a href=&#34;https://talky.io/Numerical_Estimation_of_Information_Measures&#34;&gt;talky.io&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Malena Sabaté, &lt;strong&gt;Iteratively Reweighted Flexible Krylov methods for Sparse Reconstruction&lt;/strong&gt; [&lt;a href=&#34;https://maths-of-data.github.io/MaleneSabatePoster.pdf&#34;&gt;pdf&lt;/a&gt;,&lt;a href=&#34;https://www.dropbox.com/s/eghycb6qsovntp7/Malena%20Sabat%C3%A9%20-%20MalenaSabate.mp4?dl=0&#34;&gt;video&lt;/a&gt;,&lt;a href=&#34;https://talky.io/Iteratively_Reweighted_Flexible_Krylov_methods_for_Sparse_Reconstruction&#34;&gt;talky.io&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Leah Stella, &lt;strong&gt;Tomographic Imaging of the SARS-COV2&lt;/strong&gt; [&lt;a href=&#34;https://www.dropbox.com/s/jix5x00f9q78urh/conference%20presentation.mp4?dl=0&#34;&gt;video&lt;/a&gt;,&lt;a href=&#34;https://talky.io/Tomographic_Imaging_of_the_SARS-COV2&#34;&gt;talky.io&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Cathie Wells, &lt;strong&gt;Optimising trajectories to reduce transatlantic flight emissions&lt;/strong&gt; [&lt;a href=&#34;https://maths-of-data.github.io/CathieWells-CAWellsSingleSlide.pdf&#34;&gt;pdf&lt;/a&gt;,&lt;a href=&#34;https://www.dropbox.com/s/agzfco6dpbu8gkd/Cathie%20Wells%20%20-%20CAWellsposterpresentation.mp4?dl=0&#34;&gt;video&lt;/a&gt;,&lt;a href=&#34;https://talky.io/Optimising_trajectories_to_reduce_transatlantic_flight_emissions&#34;&gt;talky.io&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h1 id=&#34;talks&#34;&gt;Talks&lt;/h1&gt;
&lt;h3 id=&#34;mathematics-of-neural-netowrks-and-deep-learning&#34;&gt;Mathematics of Neural Netowrks and Deep Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Barbara Mahler, &lt;a href=&#34;#barbara-mahler&#34;&gt;&lt;strong&gt;Contagion Maps for Manifold Learning&lt;/strong&gt;&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;Patrick Kidger, &lt;a href=&#34;#patrick-kidger&#34;&gt;&lt;strong&gt;Universal Approximation - Transposed!&lt;/strong&gt;&lt;/a&gt;, &lt;strong&gt;[&lt;a href=&#34;https://youtu.be/7IdF04PzEmY&#34;&gt;video&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1905.08539&#34;&gt;arxiv&lt;/a&gt;]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Haoran Ni, &lt;a href=&#34;#haoran-ni&#34;&gt;&lt;strong&gt;Numerical Estimation of Information Measures&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;optimisation&#34;&gt;Optimisation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Louis Sharrock, &lt;a href=&#34;#louis-sharrock&#34;&gt;&lt;strong&gt;Two-Timescale Stochastic Approximation in Continuous Time with Applications to Joint Online Parameter Estimation and Optimal Sensor Placement&lt;/strong&gt;&lt;/a&gt; &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;Nash Treetanthi, &lt;a href=&#34;#nash-treetanthi&#34;&gt;&lt;strong&gt;Uncertainty aversion in Multi-armed bandit problem&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Florentin Goyens, &lt;a href=&#34;#florentin-goyens&#34;&gt;&lt;strong&gt;Nonlinear matrix recovery&lt;/strong&gt;&lt;/a&gt;, &lt;strong&gt;[&lt;a href=&#34;https://youtu.be/2ULXV5MqdBo&#34;&gt;video&lt;/a&gt;]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Vadim Platonov, &lt;a href=&#34;#vadim-platonov&#34;&gt;&lt;strong&gt;Forward utilities and Mean-field games underrelative performance concerns&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Melanie Beckerleg, &lt;a href=&#34;#melanie-meckerleg&#34;&gt;&lt;strong&gt;Binary Matrix Completion for Recommender Systems, with applications to Drug Discovery&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;applications-of-machine-learning-in-life-sciences&#34;&gt;Applications of machine learning in life sciences&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Tamara Grossman, &lt;a href=&#34;#tamara-grossman&#34;&gt;&lt;strong&gt;Deeply Learned Spectral Total Variation Decomposition&lt;/strong&gt;&lt;/a&gt; &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;Lancelot Da-Costa, &lt;a href=&#34;#lancelo-da-costa&#34;&gt;&lt;strong&gt;A global brain theory, stochastic thermodynamics and applications to autonomous behaviour&lt;/strong&gt;&lt;/a&gt;, &lt;strong&gt;[&lt;a href=&#34;https://youtu.be/uCvzgRmLlZQ&#34;&gt;video&lt;/a&gt;]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Laura Guzmán Rincón, &lt;a href=&#34;#laura-guzman-rincon&#34;&gt;&lt;strong&gt;Outbreak detection using Bayesian hierarchical modelling and Gaussian random fields&lt;/strong&gt;&lt;/a&gt; &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bayesian-methods&#34;&gt;Bayesian methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Henry Moss, &lt;a href=&#34;#henry-moss&#34;&gt;&lt;strong&gt;BOSH: Bayesian Optimisation Sampled Hierarchically&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sam Power, &lt;a href=&#34;#sam-power&#34;&gt;&lt;strong&gt;Accelerated Sampling on Discrete Spaces with Non-Reversible Markov Processes&lt;/strong&gt;&lt;/a&gt;, &lt;strong&gt;[&lt;a href=&#34;https://youtu.be/q4DtJ1MK2SQ&#34;&gt;video&lt;/a&gt;]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Riccardo Barbano, &lt;a href=&#34;#riccardo-barbano&#34;&gt;&lt;strong&gt;Quantifying Model-Uncertainty in Inverse Problems via Bayesian Deep Gradient Descent&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;list-of-abstracts&#34;&gt;List of abstracts&lt;/h1&gt;
&lt;h3 id=&#34;barbara-mahler&#34;&gt;&lt;strong&gt;Contagion Maps for Manifold Learning&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Barbara Mahler&lt;/strong&gt;, University of Oxford&lt;/p&gt;
&lt;p&gt;Contagion maps are a family of maps that map nodes of a network to points in a high-dimensional space, based on the activations times in a threshold contagion on the network. A point cloud that is the image of such a map reflects both the structure underlying the network and the spreading behaviour of the contagion on it. Intuitively, such a point cloud exhibits features of the network&amp;rsquo;s underlying structure if the contagion spreads along that structure, an observation which suggests contagion maps as a viable manifold-learning technique. We test contagion maps as a manifold-learning tool on several different data sets, and compare its performance to that of Isomap, one of the most well-known manifold-learning algorithms. We find that, under certain conditions, contagion maps are able to reliably detect underlying manifold structure in noisy data, when Isomap is prone to noise-induced error. This consolidates contagion maps as a technique for manifold learning.&lt;/p&gt;
&lt;h3 id=&#34;patrick-kidger&#34;&gt;Universal Approximation - Transposed!&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Patrick Kidger&lt;/strong&gt;, University of Oxford&lt;/p&gt;
&lt;p&gt;The classical Universal Approximation Theorem is a foundational result in the theory of neural networks, and roughly speaking states that arbitrarily wide networks can approximate any continuous function. This is often stated as one of the reasons that neural networks actually work! This motivates a natural &amp;lsquo;dual&amp;rsquo; problem: what about networks of bounded width, but arbitrary depth? This dual problem is arguably closer to many of the networks that are used in practice. Here we address this problem by proving universal approximation for broad classes of activation functions, improving on previous work that applies only to the ReLU. In particular our results hold for polynomial activation functions, which is a qualitative difference to the classical version of the theorem. If time allows we will discuss extensions of this result, to nowhere differentiable activation functions, noncompact domains, and on the minimal width of network that can be achieved.&lt;/p&gt;
&lt;h3 id=&#34;haoran-ni&#34;&gt;&lt;strong&gt;Numerical Estimation of Information Measures&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Haoran Ni&lt;/strong&gt;, University of Warwick&lt;/p&gt;
&lt;p&gt;Entropy measures the information content of a random quantity. The closely related concept of Mutual Information (MI) between two random variables measures the reduction in uncertainty of one random variable due to information obtained from the other. As an information-theoretic quantity, MI plays an important role in many scientific disciplines. Its applications include decision trees in machine learning, independent component analysis (ICA), gene detection and expression, link prediction, topic discovery, image registration, feature selection and transformations, and channel capacity. There are theoretical and practical properties that make MI superior to other, simpler measures. At the same time, to estimate it from samples becomes a vital and difficult issue. The proposed research aims to develop efficient mutual information estimation systems with a set of estimators that produce accurate results irrespective of sample size, dimensionality, and correlation. The systems will include:
(1) Improved existing estimators; (2) New approximate k-th Nearest Neighbour (kNN) Algorithms estimator; (3) New algorithms based on fast and sparse Johnson- Lindenstrauss transforms; (4) Bias correction approaches; (5) Thorough and principled theoretical analysis of the new methods.&lt;/p&gt;
&lt;h3 id=&#34;louis-sharrock&#34;&gt;&lt;strong&gt;Two-Timescale Stochastic Approximation in Continuous Time with Applications to Joint Online Parameter Estimation and Optimal Sensor Placement&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Louis Sharrock&lt;/strong&gt;, Imperial College London&lt;/p&gt;
&lt;p&gt;In this talk, we consider the problem of joint online parameter estimation and optimal sensor placement in a partially observed diffusion process. The parameter estimation and the sensor placement are both to be performed online. We propose a solution to this problem in the form of a two-timescale stochastic gradient descent algorithm, and provide a rigorous theoretical analysis of its asymptotic properties. In particular, under suitable conditions relating to the ergodicity of the process consisting of the latent signal, the filter, and the tangent filter, we establish almost sure convergence of the online parameter estimates and the recursive optimal sensor placements to the stationary points of the asymptotic log-likelihood, and the asymptotic filter covariance, respectively. We also provide numerical examples illustrating the application of this methodology to the partially observed stochastic advection-diffusion equation.&lt;/p&gt;
&lt;h3 id=&#34;nash-treetanthi&#34;&gt;&lt;strong&gt;Uncertainty aversion in Multi-armed bandit problem&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Nash Treetanthi&lt;/strong&gt;, University of Oxford&lt;/p&gt;
&lt;p&gt;As exemplified by the Ellsberg paradox, when making decisions people often display a preference for options which carry less uncertainty. This shows &amp;lsquo;pessimistic&amp;rsquo; behaviour toward uncertainty in decision. In contrast, many bandit algorithms proposes to be &amp;lsquo;optimistic&amp;rsquo; toward uncertainty in order to benefit learning. This means that the designed algorithms may demonstrate the opposite decisions to the agent preferences.&lt;/p&gt;
&lt;p&gt;To allow both learning and uncertainty aversion, we extend the classical Gittins index theorem to a robust version using the theory of nonlinear expectation. This involves studying controls determining the filtration and a model to capture consistent uncertainty, which is unaffected from the controls, together with the relaxation of Dynamic Programming Principles.&lt;/p&gt;
&lt;p&gt;A numerical example of a binomial bandit is also considered to illustrate the interaction between an agent&amp;rsquo;s willingness to explore and uncertainty aversion arisen from the sensitivity of the statistical estimates. This shows that the preference to learn or to exploit can also be affected by the current estimate of the cost and by the precision of available estimates. The Monte-Carlo simulations also shows its benefit to make a decision when a large number of choices are available.&lt;/p&gt;
&lt;h3 id=&#34;florentin-goyens&#34;&gt;&lt;strong&gt;Nonlinear matrix recovery&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Florentin Goyens&lt;/strong&gt;, University of Oxford&lt;/p&gt;
&lt;p&gt;We address the numerical problem of recovering a partially observed high-rank matrix whose columns obey a nonlinear structure. The structures we cover include points grouped as clusters or belonging to an algebraic variety. Using a nonlinear lifting to a space of features, we reduce the problem to a constrained non-convex optimization formulation. We use a Riemannian optimization method and an alternating minimization scheme. Both approaches have first and second order variants. These methods have theoretical convergence guarantees as well as global worst-case rates. We provide extensive numerical results for the recovery of union of subspaces and clusters under entry sampling and dense Gaussian sampling.&lt;/p&gt;
&lt;h3 id=&#34;vadim-platonov&#34;&gt;&lt;strong&gt;Forward utilities and Mean-field games underrelative performance concerns&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Vadim Platonov&lt;/strong&gt;, University of Edinburgh&lt;/p&gt;
&lt;p&gt;We introduce the concept of mean field games for agents using Forward utilities to study a family of portfolio management problems under relative performance concerns. Under asset specialization of the fund managers, we solve the forward-utility finite player game and the forward-utility mean-field game. We study best response and equilibrium strategies in the single common stock asset and the asset specialization with common noise. As an application, we draw on the core features of the forward utility paradigm and discuss a problem of time-consistent mean-field dynamic model selection in sequential time-horizons.&lt;/p&gt;
&lt;h3 id=&#34;melanie-meckerleg&#34;&gt;&lt;strong&gt;Binary Matrix Completion for Recommender Systems, with applications to Drug Discovery&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Melanie Beckerleg&lt;/strong&gt;, University of Oxford&lt;/p&gt;
&lt;p&gt;Making predictions of missing entries in large databases is useful in applications including e-commerce, online services and biomedical research. In this talk I will present my findings on the use of predictive clustering to make recommendations. The starting point is a database of interactions, say for example between drug compounds and target proteins, that are thought to cause disease. Interactions can be described in terms of user clusters (i.e. drugs in the same group have similar interaction profiles). In real world
applications, many of these interactions will be unobserved. My aim is to recover not only accurate predictions of the missing entries, but also the underlying groupings. This allows us to make predictions that
are interpretable as well as accurate. I will present a convex relaxation of the problem and compare against different algorithms, in terms of numerical results and theoretical guarantees for recovery.&lt;/p&gt;
&lt;h3 id=&#34;tamara-grossman&#34;&gt;&lt;strong&gt;Deeply Learned Spectral Total Variation Decomposition&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Tamara Grossman&lt;/strong&gt;, University of Cambridge&lt;/p&gt;
&lt;p&gt;Non-linear spectral decompositions of images based on one-homogeneous functionals such as total variation have gained considerable attention. Due to their ability to extract spectral components based on size and contrast of objects, such decompositions enable image manipulation, filtering, feature transfer and image fusion among other applications. However, obtaining this decomposition involves solving multiple non-smooth optimisation problems in the classical approach and can therefore be computationally slow. In this talk, we discuss the possibility to reproduce a non-linear spectral decomposition via neural networks. Not only do we gain a computational advantage, but this approach can also be seen as a step towards studying neural networks that can decompose an image into spectral components defined by a user rather than a handcrafted functional.&lt;/p&gt;
&lt;h3 id=&#34;lancelo-da-costa&#34;&gt;&lt;strong&gt;A global brain theory, stochastic thermodynamics and applications to autonomous behaviour&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Lancelot Da-Costa&lt;/strong&gt;, Imperial College London&lt;/p&gt;
&lt;p&gt;This talk 1) reviews the free energy principle, a global theory of brain function based on variational Bayesian inference. 2) We will see how we can justify the free energy principle from Langevin dynamics. 3) Finally, I will show how we can use these ideas to simulate autonomous agents in active inference, a framework similar to Bayesian reinforcement learning.&lt;/p&gt;
&lt;h3 id=&#34;laura-guzman-rincon&#34;&gt;&lt;strong&gt;Outbreak detection using Bayesian hierarchical modelling and Gaussian random fields&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Laura Guzmán Rincón&lt;/strong&gt;, University of Warwick&lt;/p&gt;
&lt;p&gt;Identification and investigation of outbreaks remain a high priority for health authorities. Early detection of infectious diseases facilitates interventions and prevents further transmission. For that purpose, surveillance systems collect epidemiological data from patients, including location, onset time of infection and, with the recent improvements in genotyping techniques, the whole-genome sequencing of bacteria. For outbreak detection, several statistical methods have been proposed using spatial, temporal or genetic data. However, techniques mixing genomics and epidemiological factors are still underdeveloped.This talk will describe a new approach that aims to combine diverse sources of data for outbreak detection. The methodology tackles the problem as a classification task using Bayesian hierarchical models, based on an existing spatial-temporal model proposed by Spencer et al. [Spat. Spatio-temporal Epidemiol.,2(3), 173–183 (2011)]. The talk will show that the latent parameters of the model are Gaussian random fields adapted to different types of data. Moreover, the model willbe applied to study reported cases of Campylobacter infections in two regions in the UK. Two scenarios will be explained, where a temporal-genetic and a spatial-genetic model are trained using the framework proposed. The method allows us to find potential diffuse outbreaks that could not be captured using spatial-temporal methods&lt;/p&gt;
&lt;h3 id=&#34;henry-moss&#34;&gt;&lt;strong&gt;BOSH: Bayesian Optimisation Sampled Hierarchically&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Henry Moss&lt;/strong&gt;, Lancaster University&lt;/p&gt;
&lt;p&gt;Deployments of Bayesian Optimisation (BO) for functions with stochastic evaluations, such as parameter tuning via cross validation and simulation optimisation, typically optimise an average of noisy realisations of the objective function induced by a fixed collection of random seeds. However, disregarding the true objective function in this manner means that BO finds a high-precision optimum of the wrong function. To solve this problem, we propose Bayesian Optimisation by Sampling Hierarchically (BOSH), a novel BO routine pairing a hierarchical Gaussian process with a custom information-theoretic framework to generate a growing pool of seeds as the optimisation progresses. We demonstrate that BOSH provides more efficient and higher-precision optimisation than standard BO across synthetic benchmarks, simulation optimisation, reinforcement learning and hyper-parameter tuning tasks.&lt;/p&gt;
&lt;h3 id=&#34;sam-power&#34;&gt;&lt;strong&gt;Accelerated Sampling on Discrete Spaces with Non-Reversible Markov Processes&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Sam Power&lt;/strong&gt;, University of Cambridge&lt;/p&gt;
&lt;p&gt;Sampling methods for high-dimensional distributions on continuous state-spaces have come along in leaps and bounds in recent years, backed by improved theoretical results and bolstered by the widespread adoption of easy-to-use automatic differentiation packages, which enable the easy application of gradient-based methods. In contrast, simulation on discrete spaces retains a reputation for requiring bespoke, model-specific solutions, and as such, discrete models are often overlooked. In this work (joint with Jacob Vorstrup Goldman), we aim to demonstrate that sampling on discrete spaces can be treated with a similar level of generality to continuous spaces, and introduce a suite of algorithms for efficient simulation on structured discrete spaces. The algorithms build on recent developments in non-reversible, continuous-time Monte Carlo methods, and are able to out-perform previous baseline algorithms considerably in examples drawn from statistics, machine learning, physics, and beyond.&lt;/p&gt;
&lt;h3 id=&#34;riccardo-barbano&#34;&gt;&lt;strong&gt;Quantifying Model-Uncertainty in Inverse Problems via Bayesian Deep Gradient Descent&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Riccardo Barbano&lt;/strong&gt;, University College London&lt;/p&gt;
&lt;p&gt;Recent advances in reconstruction methods for inverse problems leverage powerful data-driven models, e.g., deep neural networks. These techniques have demonstrated state-of-the-art performances for several imaging tasks, but they often do not provide uncertainty on the obtained reconstructions. In this work, we develop a novel scalable data-driven knowledge-aided computational framework to quantify the model-uncertainty via Bayesian neural networks. The approach builds on and extends deep gradient descent, a recently developed greedy iterative training scheme, and recasts it within a probabilistic framework. The framework is showcased on several representative inverse imaging problems and is compared with a well-established benchmark, i.e., DnCNN.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Participants</title>
      <link>https://maths-of-data.github.io/participants/</link>
      <pubDate>Mon, 08 Jun 2020 14:21:21 +0200</pubDate>
      
      <guid>https://maths-of-data.github.io/participants/</guid>
      <description>&lt;h1 id=&#34;poster-presentations&#34;&gt;Poster presentations&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Bhavan Chahal, &lt;strong&gt;Using deep learning tools to infer house prices from Google Street View Images&lt;/strong&gt; [&lt;a href=&#34;https://www.dropbox.com/s/y2pom0lzsvw8rtw/Bhavan%20Chahal%20%20-%20bhavan_chahal_3_min_talk.mp4?dl=0&#34;&gt;video&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Adam Gothorp, &lt;strong&gt;Inverse problems for Bayesian errors-in-variables regression models&lt;/strong&gt; [&lt;a href=&#34;https://maths-of-data.github.io/Adam_Gothorp_poster_virtual_conference_mathODS_v6.pdf&#34;&gt;pdf&lt;/a&gt;,&lt;a href=&#34;https://www.dropbox.com/s/rrgsag0ku56lo0b/Adam%20Gothorp%20%20-%20Adam_Gothorp_poster_virtual_conference_mathODS.mp4?dl=0&#34;&gt;video&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Florenting Goyens, &lt;strong&gt;Nonlinear matrix recovery&lt;/strong&gt; [pdf,video]&lt;/li&gt;
&lt;li&gt;Eric Baruch Gutierrez, &lt;strong&gt;An Example of Primal-Dual Methods for Nonsmooth Large-Scale Machine Learning&lt;/strong&gt; [&lt;a href=&#34;link2&#34;&gt;video&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Haoran Ni, &lt;strong&gt;Numerical Estimation of Information Measures&lt;/strong&gt; [&lt;a href=&#34;https://maths-of-data.github.io/HaoranNi-HaoranNi_Poster.pdf&#34;&gt;pdf&lt;/a&gt;,&lt;a href=&#34;https://www.dropbox.com/s/0r3mlnfj1q3n1gi/Haoran%20Ni%20-%20HaoranNi_Poster_Presenting.mov?dl=0&#34;&gt;video&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Malena Sabaté, &lt;strong&gt;Iteratively Reweighted Flexible Krylov methods for Sparse Reconstruction&lt;/strong&gt; [&lt;a href=&#34;https://maths-of-data.github.io/MaleneSabatePoster.pdf&#34;&gt;pdf&lt;/a&gt;,&lt;a href=&#34;https://www.dropbox.com/s/eghycb6qsovntp7/Malena%20Sabat%C3%A9%20-%20MalenaSabate.mp4?dl=0&#34;&gt;video&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Leah Stella, &lt;strong&gt;Tomographic Imaging of the SARS-COV2&lt;/strong&gt; [pdf,video]&lt;/li&gt;
&lt;li&gt;Cathie Wells, &lt;strong&gt;Optimising trajectories to reduce transatlantic flight emissions&lt;/strong&gt; [&lt;a href=&#34;https://maths-of-data.github.io/CathieWells-CAWellsSingleSlide.pdf&#34;&gt;pdf&lt;/a&gt;,&lt;a href=&#34;https://www.dropbox.com/s/agzfco6dpbu8gkd/Cathie%20Wells%20%20-%20CAWellsposterpresentation.mp4?dl=0&#34;&gt;video&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Schedule</title>
      <link>https://maths-of-data.github.io/program/</link>
      <pubDate>Mon, 01 Jun 2020 14:27:41 +0200</pubDate>
      
      <guid>https://maths-of-data.github.io/program/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;a href=&#34;https://maths-of-data.github.io/MathODS_talk_list.pdf&#34;&gt;Click here for the detailed list of talks.&lt;/a&gt;
&lt;a href=&#34;https://maths-of-data.github.io/MathODS_Schedule.pdf&#34;&gt;&lt;figure class=&#34;schedule&#34;&gt;
    &lt;img src=&#34;https://maths-of-data.github.io/program_img.jpg&#34; width=&#34;70%&#34;/&gt; 
&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h1 id=&#34;types-of-contributions&#34;&gt;Types of contributions&lt;/h1&gt;
&lt;p&gt;MathODS will provide a venue for two types of contribution.&lt;/p&gt;
&lt;h3 id=&#34;contributed-talk-20-min--10-min-questions-single-track&#34;&gt;Contributed talk (20 min. + 10 min. questions, single track)&lt;/h3&gt;
&lt;p&gt;Talks will be &lt;strong&gt;20 min.&lt;/strong&gt; long followed by up to &lt;strong&gt;10 min.&lt;/strong&gt; of questions and presented in a &lt;strong&gt;single track session&lt;/strong&gt; for each of the four topic areas.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h3 id=&#34;poster-presentation-50-min-multi-track&#34;&gt;Poster presentation (50 min., multi track)&lt;/h3&gt;
&lt;p&gt;Each poster contributor submits a short &lt;strong&gt;3 minute video&lt;/strong&gt; presentation two days before the conference. The video can be just a recording of talking through slides or in front of a physical poster.&lt;/p&gt;
&lt;p&gt;During the conference, posters will be presented in break-out rooms where there will be an oportunity to ask the presenters questions about their work in a &lt;strong&gt;multi track session&lt;/strong&gt;. Poster sessions are aimed to allow for more interactive presentations and can go beyond showing a PDF of a poster, e.g. can include &lt;strong&gt;code demonstrations&lt;/strong&gt;, etc.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Access</title>
      <link>https://maths-of-data.github.io/access/</link>
      <pubDate>Mon, 20 Apr 2020 14:21:14 +0200</pubDate>
      
      <guid>https://maths-of-data.github.io/access/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
</description>
    </item>
    
  </channel>
</rss>